#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§ç”¨æ³•ç¤ºä¾‹
æ¼”ç¤ºå‚æ•°è°ƒèŠ‚ã€æ€§èƒ½å¯¹æ¯”ã€ç®—æ³•ç»„åˆç­‰é«˜çº§åŠŸèƒ½
"""

import sys
import os
# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ä¼˜åŒ–æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import matplotlib.pyplot as plt
import time
from one_dim import OneDimensionalOptimization
from two_dim import TwoDimensionalOptimization
from cons_optimiz import ConstrainedOptimization

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def parameter_sensitivity_study():
    """å‚æ•°æ•æ„Ÿæ€§ç ”ç©¶"""
    print("=" * 70)
    print("å‚æ•°æ•æ„Ÿæ€§ç ”ç©¶")
    print("=" * 70)
    
    # æµ‹è¯•å‡½æ•°
    def test_func(x):
        return (x[0] - 2)**2 + (x[1] - 1)**2
    
    x0 = np.array([0.0, 0.0])
    true_optimum = np.array([2.0, 1.0])
    
    # æµ‹è¯•ä¸åŒçš„å®¹å·®è®¾ç½®
    tolerances = [1e-4, 1e-6, 1e-8, 1e-10]
    
    print("å®¹å·®å¯¹æ”¶æ•›çš„å½±å“:")
    print("-" * 50)
    
    for tol in tolerances:
        optimizer = TwoDimensionalOptimization(tolerance=tol, max_iterations=100)
        
        start_time = time.time()
        x_opt, iterations, _ = optimizer.conjugate_gradient(test_func, x0)
        end_time = time.time()
        
        error = np.linalg.norm(x_opt - true_optimum)
        runtime = end_time - start_time
        
        print(f"å®¹å·® {tol:.0e}: è¯¯å·® = {error:.8f}, è¿­ä»£ = {iterations:2d}, "
              f"æ—¶é—´ = {runtime:.4f}s")

def algorithm_comparison_study():
    """ç®—æ³•æ€§èƒ½å¯¹æ¯”ç ”ç©¶"""
    print("\n" + "=" * 70)
    print("ç®—æ³•æ€§èƒ½å¯¹æ¯”ç ”ç©¶")
    print("=" * 70)
    
    # å®šä¹‰å¤šä¸ªæµ‹è¯•å‡½æ•°
    test_functions = {
        'äºŒæ¬¡å‡½æ•°': (lambda x: (x[0]-1)**2 + (x[1]-2)**2, np.array([1.0, 2.0])),
        'Rosenbrock': (lambda x: 100*(x[1]-x[0]**2)**2 + (1-x[0])**2, np.array([1.0, 1.0])),
        'æ¤­åœ†å‡½æ•°': (lambda x: 2*x[0]**2 + x[1]**2, np.array([0.0, 0.0])),
        'åŒå³°å‡½æ•°': (lambda x: (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2, np.array([3.0, 2.0]))
    }
    
    optimizer = TwoDimensionalOptimization(tolerance=1e-6, max_iterations=200)
    
    methods = [
        ('æœ€é€Ÿä¸‹é™æ³•', optimizer.steepest_descent),
        ('å…±è½­æ¢¯åº¦æ³•', optimizer.conjugate_gradient),
        ('æ‹Ÿç‰›é¡¿æ³•', optimizer.dfp_method),
        ('é˜»å°¼ç‰›é¡¿æ³•', optimizer.damped_newton_method)
    ]
    
    results = {}
    
    for func_name, (func, true_opt) in test_functions.items():
        print(f"\næµ‹è¯•å‡½æ•°: {func_name}")
        print("-" * 40)
        
        x0 = np.array([-1.0, 1.0])  # ç»Ÿä¸€åˆå§‹ç‚¹
        func_results = []
        
        for method_name, method in methods:
            try:
                start_time = time.time()
                x_opt, iterations, path = method(func, x0)
                end_time = time.time()
                
                error = np.linalg.norm(x_opt - true_opt)
                runtime = end_time - start_time
                final_value = func(x_opt)
                
                func_results.append({
                    'method': method_name,
                    'error': error,
                    'iterations': iterations,
                    'runtime': runtime,
                    'final_value': final_value,
                    'path_length': len(path)
                })
                
                print(f"{method_name:15s}: è¯¯å·®={error:.6f}, è¿­ä»£={iterations:3d}, "
                      f"æ—¶é—´={runtime:.4f}s, f*={final_value:.6f}")
                
            except Exception as e:
                print(f"{method_name:15s}: å¤±è´¥ - {str(e)[:30]}")
        
        results[func_name] = func_results
    
    return results

def robust_optimization_study():
    """é²æ£’æ€§ä¼˜åŒ–ç ”ç©¶"""
    print("\n" + "=" * 70)
    print("é²æ£’æ€§ä¼˜åŒ–ç ”ç©¶ï¼ˆä¸åŒåˆå§‹ç‚¹ï¼‰")
    print("=" * 70)
    
    def rosenbrock(x):
        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2
    
    optimizer = TwoDimensionalOptimization(tolerance=1e-6, max_iterations=100)
    true_optimum = np.array([1.0, 1.0])
    
    # å¤šä¸ªä¸åŒçš„åˆå§‹ç‚¹
    initial_points = [
        np.array([-2.0, 2.0]),
        np.array([0.0, 0.0]),
        np.array([2.0, -1.0]),
        np.array([-1.0, -1.0]),
        np.array([1.5, 3.0])
    ]
    
    methods = [
        ('å…±è½­æ¢¯åº¦æ³•', optimizer.conjugate_gradient),
        ('æ‹Ÿç‰›é¡¿æ³•', optimizer.dfp_method),
        ('é˜»å°¼ç‰›é¡¿æ³•', optimizer.damped_newton_method)
    ]
    
    print("åˆå§‹ç‚¹å¯¹æ”¶æ•›çš„å½±å“ (Rosenbrockå‡½æ•°):")
    print("-" * 60)
    
    for i, x0 in enumerate(initial_points):
        print(f"\nåˆå§‹ç‚¹ {i+1}: ({x0[0]:5.1f}, {x0[1]:5.1f})")
        
        for method_name, method in methods:
            try:
                x_opt, iterations, _ = method(rosenbrock, x0)
                error = np.linalg.norm(x_opt - true_optimum)
                success = "æˆåŠŸ" if error < 1e-3 else "å¤±è´¥"
                
                print(f"  {method_name:15s}: è¯¯å·®={error:.6f}, è¿­ä»£={iterations:3d}, {success}")
                
            except Exception:
                print(f"  {method_name:15s}: ç®—æ³•å‘æ•£")

def constraint_handling_comparison():
    """çº¦æŸå¤„ç†æ–¹æ³•å¯¹æ¯”"""
    print("\n" + "=" * 70)
    print("çº¦æŸå¤„ç†æ–¹æ³•å¯¹æ¯”")
    print("=" * 70)
    
    # å®šä¹‰çº¦æŸä¼˜åŒ–é—®é¢˜
    def objective(x):
        return (x[0] - 3)**2 + (x[1] - 2)**2
    
    def eq_constraint(x):
        return x[0]**2 + x[1]**2 - 5  # xÂ² + yÂ² = 5
    
    def ineq_constraint(x):
        return x[0] + x[1] - 1  # x + y >= 1
    
    optimizer = ConstrainedOptimization(tolerance=1e-6, max_iterations=50)
    x0 = np.array([2.0, 1.0])
    
    print("é—®é¢˜: min (x-3)Â² + (y-2)Â², s.t. xÂ²+yÂ²=5, x+yâ‰¥1")
    print(f"åˆå§‹ç‚¹: ({x0[0]}, {x0[1]})")
    print("-" * 50)
    
    methods = [
        ('çº¦æŸåæ ‡è½®æ¢æ³•', lambda: optimizer.constrained_coordinate_descent(
            objective, x0, [eq_constraint], [ineq_constraint])),
        ('æ‹‰æ ¼æœ—æ—¥æ³•', lambda: optimizer.lagrange_method(
            objective, x0, [eq_constraint])),
        ('æƒ©ç½šå‡½æ•°æ³•', lambda: optimizer.penalty_method(
            objective, x0, [eq_constraint], [ineq_constraint]))
    ]
    
    for method_name, method_func in methods:
        try:
            start_time = time.time()
            x_opt, iterations, path = method_func()
            end_time = time.time()
            
            runtime = end_time - start_time
            final_value = objective(x_opt)
            is_feasible = optimizer.is_feasible(x_opt, [eq_constraint], [ineq_constraint])
            
            print(f"{method_name:20s}: f*={final_value:.6f}, è¿­ä»£={iterations:3d}, "
                  f"æ—¶é—´={runtime:.4f}s, å¯è¡Œ={'æ˜¯' if is_feasible else 'å¦'}")
            
        except Exception as e:
            print(f"{method_name:20s}: å¤±è´¥ - {str(e)[:30]}")

def adaptive_parameter_optimization():
    """è‡ªé€‚åº”å‚æ•°ä¼˜åŒ–"""
    print("\n" + "=" * 70)
    print("è‡ªé€‚åº”å‚æ•°ä¼˜åŒ–ç¤ºä¾‹")
    print("=" * 70)
    
    def difficult_function(x):
        """éš¾ä¼˜åŒ–çš„å‡½æ•°ï¼ˆæ¡ä»¶æ•°å¾ˆå¤§ï¼‰"""
        return 100 * x[0]**2 + x[1]**2
    
    x0 = np.array([1.0, 10.0])
    true_optimum = np.array([0.0, 0.0])
    
    print("éš¾ä¼˜åŒ–å‡½æ•°: f(x,y) = 100xÂ² + yÂ² (æ¡ä»¶æ•°=100)")
    print("æ¯”è¾ƒæ ‡å‡†å‚æ•°å’Œè°ƒæ•´å‚æ•°çš„æ•ˆæœ:")
    print("-" * 50)
    
    # æ ‡å‡†å‚æ•°
    optimizer1 = TwoDimensionalOptimization(tolerance=1e-6, max_iterations=200)
    start_time = time.time()
    x_opt1, iter1, path1 = optimizer1.steepest_descent(difficult_function, x0)
    time1 = time.time() - start_time
    error1 = np.linalg.norm(x_opt1 - true_optimum)
    
    # è°ƒæ•´å‚æ•°ï¼ˆæ›´ä¸¥æ ¼çš„å®¹å·®ï¼Œæ›´å¤šè¿­ä»£ï¼‰
    optimizer2 = TwoDimensionalOptimization(tolerance=1e-8, max_iterations=500)
    start_time = time.time()
    x_opt2, iter2, path2 = optimizer2.conjugate_gradient(difficult_function, x0)
    time2 = time.time() - start_time
    error2 = np.linalg.norm(x_opt2 - true_optimum)
    
    print(f"æœ€é€Ÿä¸‹é™æ³•(æ ‡å‡†): è¯¯å·®={error1:.8f}, è¿­ä»£={iter1:3d}, æ—¶é—´={time1:.4f}s")
    print(f"å…±è½­æ¢¯åº¦æ³•(è°ƒæ•´): è¯¯å·®={error2:.8f}, è¿­ä»£={iter2:3d}, æ—¶é—´={time2:.4f}s")
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    path1_array = np.array(path1)
    plt.plot(path1_array[:, 0], path1_array[:, 1], 'ro-', markersize=3, label='æœ€é€Ÿä¸‹é™æ³•')
    plt.plot(0, 0, 'g*', markersize=15, label='çœŸå®æœ€ä¼˜ç‚¹')
    plt.title('æœ€é€Ÿä¸‹é™æ³•è·¯å¾„')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    path2_array = np.array(path2)
    plt.plot(path2_array[:, 0], path2_array[:, 1], 'bo-', markersize=3, label='å…±è½­æ¢¯åº¦æ³•')
    plt.plot(0, 0, 'g*', markersize=15, label='çœŸå®æœ€ä¼˜ç‚¹')
    plt.title('å…±è½­æ¢¯åº¦æ³•è·¯å¾„')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def hybrid_optimization_approach():
    """æ··åˆä¼˜åŒ–æ–¹æ³•"""
    print("\n" + "=" * 70)
    print("æ··åˆä¼˜åŒ–æ–¹æ³•ç¤ºä¾‹")
    print("=" * 70)
    
    def complex_function(x):
        """å¤æ‚çš„å¤šå³°å‡½æ•°"""
        return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2 + 0.1*np.sin(10*x[0])*np.sin(10*x[1])
    
    x0 = np.array([0.0, 0.0])
    
    print("æ··åˆç­–ç•¥: å…ˆç”¨å…¨å±€æœç´¢ï¼Œå†ç”¨å±€éƒ¨ä¼˜åŒ–")
    print("-" * 50)
    
    optimizer = TwoDimensionalOptimization(tolerance=1e-6, max_iterations=50)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šç²—ç•¥æœç´¢ï¼ˆæœ€é€Ÿä¸‹é™æ³•ï¼‰
    print("ç¬¬ä¸€é˜¶æ®µï¼šå…¨å±€ç²—ç•¥æœç´¢")
    x_coarse, iter1, path1 = optimizer.steepest_descent(complex_function, x0)
    print(f"ç²—ç•¥æœç´¢ç»“æœ: x=({x_coarse[0]:.4f}, {x_coarse[1]:.4f}), "
          f"f={complex_function(x_coarse):.6f}, è¿­ä»£={iter1}")
    
    # ç¬¬äºŒé˜¶æ®µï¼šç²¾ç»†ä¼˜åŒ–ï¼ˆç‰›é¡¿æ³•ï¼‰
    print("\nç¬¬äºŒé˜¶æ®µï¼šå±€éƒ¨ç²¾ç»†ä¼˜åŒ–")
    x_fine, iter2, path2 = optimizer.damped_newton_method(complex_function, x_coarse)
    print(f"ç²¾ç»†ä¼˜åŒ–ç»“æœ: x=({x_fine[0]:.4f}, {x_fine[1]:.4f}), "
          f"f={complex_function(x_fine):.6f}, è¿­ä»£={iter2}")
    
    # ä¸å•ä¸€æ–¹æ³•æ¯”è¾ƒ
    print("\nå•ä¸€æ–¹æ³•å¯¹æ¯”:")
    x_single, iter_single, _ = optimizer.conjugate_gradient(complex_function, x0)
    print(f"å…±è½­æ¢¯åº¦æ³•   : x=({x_single[0]:.4f}, {x_single[1]:.4f}), "
          f"f={complex_function(x_single):.6f}, è¿­ä»£={iter_single}")
    
    total_iterations = iter1 + iter2
    print(f"\næ··åˆæ–¹æ³•æ€»è¿­ä»£: {total_iterations}, å•ä¸€æ–¹æ³•: {iter_single}")
    improvement = complex_function(x_single) - complex_function(x_fine)
    print(f"å‡½æ•°å€¼æ”¹è¿›: {improvement:.6f}")

def performance_profiling():
    """æ€§èƒ½å‰–æ"""
    print("\n" + "=" * 70)
    print("æ€§èƒ½å‰–æ")
    print("=" * 70)
    
    def benchmark_function(x):
        """åŸºå‡†æµ‹è¯•å‡½æ•°"""
        return (x[0] - 1)**2 + (x[1] - 1)**2
    
    x0 = np.array([0.0, 0.0])
    n_runs = 10  # è¿è¡Œæ¬¡æ•°
    
    optimizer = TwoDimensionalOptimization(tolerance=1e-6, max_iterations=100)
    methods = [
        ('æœ€é€Ÿä¸‹é™æ³•', optimizer.steepest_descent),
        ('å…±è½­æ¢¯åº¦æ³•', optimizer.conjugate_gradient),
        ('æ‹Ÿç‰›é¡¿æ³•', optimizer.dfp_method)
    ]
    
    print(f"åŸºå‡†æµ‹è¯• (å¹³å‡ {n_runs} æ¬¡è¿è¡Œ):")
    print("-" * 40)
    
    for method_name, method in methods:
        times = []
        iterations = []
        
        for _ in range(n_runs):
            start_time = time.time()
            _, iter_count, _ = method(benchmark_function, x0)
            end_time = time.time()
            
            times.append(end_time - start_time)
            iterations.append(iter_count)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        avg_iter = np.mean(iterations)
        
        print(f"{method_name:15s}: å¹³å‡æ—¶é—´={avg_time:.6f}Â±{std_time:.6f}s, "
              f"å¹³å‡è¿­ä»£={avg_iter:.1f}")

if __name__ == "__main__":
    print("ğŸš€ é«˜çº§ç”¨æ³•ç¤ºä¾‹æ¼”ç¤º")
    print("=" * 70)
    
    # è¿è¡Œå„ç§é«˜çº§åŠŸèƒ½æ¼”ç¤º
    parameter_sensitivity_study()
    algorithm_comparison_study()
    robust_optimization_study()
    constraint_handling_comparison()
    
    print("\n" + "="*70)
    print("æ˜¯å¦è¿è¡Œè‡ªé€‚åº”å‚æ•°ä¼˜åŒ–å’Œæ··åˆæ–¹æ³•æ¼”ç¤ºï¼Ÿ(éœ€è¦æ˜¾ç¤ºå›¾å½¢) (y/n)")
    choice = input().lower()
    if choice == 'y' or choice == 'yes':
        adaptive_parameter_optimization()
        hybrid_optimization_approach()
    
    print("\n" + "="*70)
    print("æ˜¯å¦è¿è¡Œæ€§èƒ½å‰–æï¼Ÿ(y/n)")
    choice = input().lower()
    if choice == 'y' or choice == 'yes':
        performance_profiling()
    
    print("\nç¨‹åºç»“æŸï¼")
    print("è¿™äº›é«˜çº§åŠŸèƒ½å¯ä»¥å¸®åŠ©æ‚¨:")
    print("1. é€‰æ‹©åˆé€‚çš„ç®—æ³•å‚æ•°")
    print("2. å¯¹æ¯”ä¸åŒç®—æ³•çš„æ€§èƒ½")
    print("3. å¤„ç†å¤æ‚çš„ä¼˜åŒ–é—®é¢˜")
    print("4. ç»„åˆå¤šç§æ–¹æ³•æé«˜æ•ˆæœ") 